{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e3dbab-b2c7-4564-8973-8edfca5c1b08",
   "metadata": {},
   "source": [
    "## Install pymupdf before running the code! ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824496d7-9e7b-4ea1-9882-633e194176bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b2dea-4478-49b6-9bbb-fc7abf1870ef",
   "metadata": {},
   "source": [
    "## Main code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12988be1-58c9-4695-ace2-c2caa1bf0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "from math import sqrt\n",
    "\n",
    "# step 2: bag of words creation\n",
    "# fucntion converts text to single-line bag of words\n",
    "# given text in sentences and list of words to include in the bag\n",
    "# returns vector representing bag of words\n",
    "def vectorize_text(sentences, words):\n",
    "    vector = [0 for _ in range(len(words))]\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word in words:\n",
    "                vector[words.index(word)] += 1\n",
    "    return vector\n",
    "\n",
    "# step 1: parsing pdf to text and its tokenization\n",
    "def parse_file(file):\n",
    "    # converting pdf to python str page by page\n",
    "    pdf = pymupdf.open(file)\n",
    "    sentences = []\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        text_from_page = page.get_text().replace(\"ﬁ\", \"fi\").replace(\"ﬂ\", \"fl\").lower() # replace f-ligaments according to advice in chat\n",
    "        text_from_page = re.sub(\"[^\\w|\\.]\", \" \", text_from_page) # replacing any non-character symbol except full stop with spaces\n",
    "        text_from_page = re.sub(\"\\d+\\s*\", \" \", text_from_page) # replacing all digits with spaces\n",
    "        text_from_page = re.sub(\"\\s{2,}\", \" \", text_from_page) # getting rid of extra spaces\n",
    "        text_in_sentences = re.sub(\"\\s\\.\", \".\", text_from_page).split(\".\") # getting rid of extra spaces before full stops\n",
    "        for sentence in text_in_sentences:\n",
    "            # collecting all text in sentences\n",
    "            sentences.append(sentence)\n",
    "            for word in sentence.split():\n",
    "                if len(word) <= 2 or word in stopwords: # skip short words and words from stopwords\n",
    "                    continue\n",
    "                if dict_for_bag.get(word) is None: # count word in dict for bag of words\n",
    "                    dict_for_bag[word] = 0\n",
    "                dict_for_bag[word] += 1\n",
    "    return sentences\n",
    "\n",
    "# cosine similarity without numpy\n",
    "def cosine_similarity(first_vector, second_vector):\n",
    "    if len(first_vector) != len(second_vector):\n",
    "        print(\"Error: vectors have different dimmentionality\")\n",
    "        return None\n",
    "    dot_product = sum([first_vector[i] * second_vector[i] for i in range(len(first_vector))])\n",
    "    len_first = sqrt(sum([x**2 for x in first_vector]))\n",
    "    len_second = sqrt(sum([x**2 for x in second_vector]))\n",
    "    return dot_product / (len_first * len_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dd5609-7617-49df-8daa-a5c9b8e39080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words size: 392\n",
      "Cosine similarity of two articles: 0.3444794116278851\n"
     ]
    }
   ],
   "source": [
    "# stopwords were taken from nltk list of stopwords, since this is high quality source \n",
    "stopwords = ['i','me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",\"you'd\",'your','yours','yourself','yourselves','he','him','his','himself','she',\"she's\",'her','hers','herself','it',\"it's\",'its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that',\"that'll\",'these','those','am','is','are','was','were','werebe','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don',\"don't\",'should',\"should've\",'now','d','ll','m','o','re','ve','y','ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",'ma','mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"]\n",
    "\n",
    "# make sure to refactor names of articel files and put them next to the notebook\n",
    "dist_rep = \"Distributed Representations.pdf\"\n",
    "att_is_all = \"Attention is All You Need.pdf\"\n",
    "\n",
    "\n",
    "dict_for_bag = {}\n",
    "# parsing texts\n",
    "dist_rep_in_sent = parse_file(dist_rep)\n",
    "att_is_all_in_sent = parse_file(att_is_all)\n",
    "\n",
    "# step 2: shrinking bag of words\n",
    "# deleting all the words with less than 4 appearances\n",
    "# this leads to bag of words shrink to approximately 400 words\n",
    "amount_of_appearances_to_stay = 4\n",
    "keys = list(dict_for_bag.keys())\n",
    "for key in keys:\n",
    "    if dict_for_bag[key] < amount_of_appearances_to_stay:\n",
    "        del dict_for_bag[key]\n",
    "        \n",
    "word_list = list(dict_for_bag.keys())\n",
    "# check out words from bag of words, if you want\n",
    "#print(f\"Bag of words: {word_list}\")\n",
    "print(f\"Bag of words size: {len(word_list)}\")\n",
    "\n",
    "# step 3: generating vectors for both texts and calculating cosine similarity\n",
    "dist_rep_vec = vectorize_text(dist_rep_in_sent, word_list)\n",
    "att_is_all = vectorize_text(att_is_all_in_sent, word_list)\n",
    "\n",
    "print(f\"Cosine similarity of two articles: {cosine_similarity(dist_rep_vec, att_is_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4bed2f-b0b2-4abb-9014-5d32da27c5bf",
   "metadata": {},
   "source": [
    "## Conclusion ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823251e-ec1f-4583-a2f6-c44d578d7d5c",
   "metadata": {},
   "source": [
    "Step 4: interpret cosine similarity\n",
    "\n",
    "Cosine similarity value ranges between 0 and 1, with 0 corresponds to completly different unlike texts and 1 indicates that texts are identical. In this case, I got cosine similarity 0.344, which means that the articles discuss related topics or themes, but they likely have distinct perspectives, writing styles, or focus areas. I think, articles share some conceptual overlap and vocabulary, but they are not highly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35e76b-5bb7-4504-812a-20c669b700d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
